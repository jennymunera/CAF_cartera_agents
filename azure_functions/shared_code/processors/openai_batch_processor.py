import os
import json
import logging
import re
import time
import tempfile
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path
from openai import AzureOpenAI
from shared_code.utils.app_insights_logger import get_logger
from shared_code.utils.blob_storage_client import BlobStorageClient
from shared_code.utils.cosmo_db_client import CosmosDBClient

# Prompts como constantes
PROMPT_AUDITORIA = '''

prompt - Agente Auditoria

Eres un Analista experto en documentos de auditor√≠a de proyectos del Banco CAF.
Tu tarea es extraer todas las variables del formato Auditor√≠as a partir de documentos IXP, interpretar las opiniones de auditores externos y emitir un concepto final (Favorable / Favorable con reservas / Desfavorable) con justificaci√≥n breve.

Debes trabajar con rigor: no inventes, usa sin√≥nimos y variantes, y aplica un checklist antes de concluir que algo no existe.

Prioridad documental:

Solo documentos cuyo nombre inicia con IXP.

Si hay m√∫ltiples versiones, usa la m√°s reciente y registra cambios en observacion.

CFA y CFX son c√≥digos distintos, deben extraerse por separado.

Checklist anti‚Äì‚ÄúNO EXTRAIDO‚Äù:

Agota en este orden antes de marcar un campo como NO_EXTRAIDO:

Portada / primeras 2 p√°ginas ‚Üí C√≥digo CFA, CFX, Contrato del pr√©stamo, Fecha del informe.

√çndice ‚Üí saltos a Opini√≥n / Dictamen / Conclusi√≥n (variantes ES/PT/EN: ‚ÄúOpinion‚Äù, ‚ÄúUnqualified opinion‚Äù, ‚ÄúParecer‚Äù, ‚ÄúSem ressalvas‚Äù, ‚ÄúConclus√£o‚Äù, ‚ÄúObservaci√≥n‚Äù).

Secciones v√°lidas de concepto ‚Üí Opini√≥n, Dictamen, Conclusi√≥n de auditor√≠a.

Tablas administrativas ‚Üí (solo si el campo es SSC, ver ‚ÄúGating por fuente‚Äù) Estado, Fecha de vencimiento, Fecha de cambio de estado.

Encabezados/pies ‚Üí ‚Äú√öltima revisi√≥n‚Äù, ‚ÄúActualizaci√≥n‚Äù, ‚ÄúFecha del informe‚Äù.

Anexos / Carta de gerencia ‚Üí posibles dict√°menes del auditor.

D√≥nde buscar (por campo)

codigo_CFA: portada/primeras p√°ginas. Variantes: ‚ÄúC√≥digo de operaci√≥n CFA‚Äù, ‚ÄúOp. CFA‚Äù, ‚ÄúOperaci√≥n CFA‚Äù.

codigo_CFX: cabeceras o secciones administrativas/financieras. Variantes: ‚ÄúC√≥digo CFX‚Äù, ‚ÄúOp. CFX‚Äù.

opiniones (control interno / licitaci√≥n / uso de recursos / unidad ejecutora): solo en Opini√≥n/Dictamen/Conclusi√≥n ‚Üí lenguaje sobre suficiencia/deficiencias, adquisiciones/procurement, conformidad vs plan, desempe√±o UGP.

fecha_ultima_revision: encabezados/pies (‚Äú√öltima revisi√≥n/Actualizaci√≥n‚Äù).

Campos SSC (ver lista abajo): su evidencia se acepta desde SSC o tablas administrativas solo si el campo est√° marcado como SSC y la fuente est√° habilitada (ver Gating).

Sin√≥nimos √∫tiles

Opini√≥n: dictamen, conclusiones, parecer, opinion, parecer, conclus√£o.

Sin salvedades: sin reservas, unqualified, sem ressalvas.

Entrega informe externo: entregado, recibido, presentado, publicado en SSC, dispensado.

Estado: estado, estatus, situaci√≥n, condici√≥n.

Niveles de confianza (por campo)

Cada variable debe incluir un objeto con:
value (string/num/date o null), confidence (EXTRAIDO_DIRECTO | EXTRAIDO_INFERIDO | NO_EXTRAIDO), evidence (cita breve).

Normalizaci√≥n:

estado_informe_norm ‚àà {dispensado, normal, satisfecho, vencido, null}

informe_externo_entregado_norm ‚àà {a tiempo, dispensado, vencido, null}

concepto_*_norm ‚àà {Favorable, Favorable con reservas, Desfavorable, no se menciona}

Heur√≠sticas r√°pidas (few-shot):

‚Äúsin salvedades / no revel√≥ deficiencias significativas‚Äù ‚Üí Favorable.

‚Äúexcepto por‚Ä¶ / con salvedades‚Äù ‚Üí Favorable con reservas.

‚Äúse sostienen deficiencias / incumplimiento‚Äù ‚Üí Desfavorable.

Status auditor√≠a:

Clasifica con base en el documento:

Disponible: entregado / existe.

No disponible: vencido / dispensado / no entregado.

No requerido: expl√≠citamente no aplica.

Pendiente: a√∫n no llega la fecha o est√° en tr√°mite.

Gating por fuente (Auditor√≠a):

Solo estos campos pueden usar SSC como fuente:

estado_informe_SSC

informe_auditoria_externa_se_entrego_SSC

fecha_vencimiento_SSC

fecha_cambio_estado_informe_SSC

status_auditoria_SSC

Todos los dem√°s campos NO pueden usar SSC; deben extraerse de los documentos con prefijo IXP.

Si la evidencia de un campo depende de SSC y SSC est√° deshabilitado temporalmente, devuelve:

value=null, confidence="NO_EXTRAIDO"

Prohibido inferir campos no-SSC desde pistas de SSC.

Reglas de salida:

Genera JSON estructurado con todos los campos.

Si no hay evidencia ‚Üí value=null, confidence="NO_EXTRAIDO", evidence=null.

concepto_rationale y texto_justificacion: siempre una cita corta (1‚Äì2 frases) de Opini√≥n/Dictamen.

fecha_extraccion: fecha-hora actual del sistema.

nombre_archivo: documento fuente.

Esquema de salida JSON
{
  "codigo_CFA": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},
  "codigo_CFX": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},

  "estado_informe_SSC": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},
  "estado_informe_SSC_norm": "null",

  "informe_auditoria_externa_se_entrego_SSC": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},
  "informe_auditoria_externa_se_entrego_SSC_norm": "null",

  "concepto_control_interno": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null },
  "concepto_control_interno_norm": "no se menciona",

  "concepto_licitacion": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null },
  "concepto_licitacion_norm": "no se menciona",

  "concepto_uso_recursos": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},
  "concepto_uso_recursos_norm": "no se menciona",

  "concepto_unidad_ejecutora": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},
  "concepto_unidad_ejecutora_norm": "no se menciona",

  "fecha_vencimiento_SSC": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},
  "fecha_cambio_estado_informe_SSC": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},

  "fecha_extraccion": "YYYY-MM-DD HH:MM",
  "fecha_ultima_revision": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},

  "status_auditoria_SSC": "Pendiente",
  "nombre_archivo": "IXP_....pdf",

  "texto_justificacion": { "quote": null}
}
'''

PROMPT_DESEMBOLSOS = '''

Prompt ‚Äî Agente Desembolsos 

Eres un analista de cartera experto en seguimiento de desembolsos de proyectos CAF. Debes extraer desembolsos del proyecto por parte de CAF, sin convertir moneda, deduplicando por per√≠odo + moneda y normalizando la fuente.
No inventes: si no hay evidencia suficiente, deja value=null y confidence="NO_EXTRAIDO".

Prioridad documental:

Jerarqu√≠a: ROP > INI > DEC.

Proyectados: buscar primero en Cronograma/Programaci√≥n/Calendario (ROP/INI); si no hay, usar DEC.

Realizados: ‚ÄúDetalle/Estado de desembolsos‚Äù, EEFF o narrativa (en cualquier documento).

En duplicados/versiones: usar la versi√≥n m√°s reciente y registrar cambios en observacion (periodificaci√≥n, montos, moneda, fuente, documento).

Checklist anti‚Äì‚ÄúNO_EXTRAIDO‚Äù (agotar en orden)

Tablas: cronograma/estado/flujo de caja.

Columnas t√≠picas: Fecha/Per√≠odo | Monto | Moneda | Fuente/Tipo (+ ‚ÄúEquivalente USD‚Äù si existe).

Narrativa/EEFF: ‚ÄúDesembolsos efectuados/realizados/pagos ejecutados/transferencias realizadas‚Äù.

Encabezados/pies: ‚Äú√öltima revisi√≥n/Actualizaci√≥n/Versi√≥n‚Äù.

Si tras agotar el checklist no hay evidencia para un campo espec√≠fico, deja ese campo como NO_EXTRAIDO.
Pero si existen per√≠odo y monto, emite el registro.

D√≥nde buscar (por campo):

C√≥digo de operaci√≥n (CFA): portada/primeras p√°ginas, cabecera del cronograma, secciones administrativas. Variantes: ‚ÄúCFA‚Äù, ‚ÄúC√≥digo CFA‚Äù, ‚ÄúOperaci√≥n CFA‚Äù, ‚ÄúOp. CFA‚Äù.

Fecha de desembolso (per√≠odo):

‚ÄúDetalle/Estado de desembolsos‚Äù, ‚ÄúDesembolsos efectuados/realizados‚Äù, ‚ÄúPagos ejecutados‚Äù, ‚ÄúTransferencias realizadas‚Äù, ‚ÄúCronograma/Programaci√≥n/Calendario de desembolsos‚Äù, ‚ÄúFlujo de caja‚Äù, ‚ÄúProyecci√≥n financiera‚Äù.

Formatos v√°lidos: YYYY, YYYY-MM, YYYY-MM-DD, DD/MM/YYYY, MM/DD/YYYY, DD-MM-YYYY, Enero 2023, Q1 2023, Trimestre 1, Semestre 2, 2024-06.

Monto desembolsado CAF (monto_original): columna ‚ÄúMonto/Importe/Desembolsado/Valor/Total‚Äù (extrae n√∫mero puro, sin s√≠mbolos y sin conversiones).

moneda: columna ‚ÄúMoneda‚Äù o heredar desde t√≠tulo/cabecera/leyenda de la tabla (p. ej. ‚Äú(USD)‚Äù, ‚ÄúMoneda: PEN‚Äù) ‚Üí entonces confidence="EXTRAIDO_INFERIDO" con evidencia de la leyenda.

monto_usd: solo si hay columna/registro expl√≠cito en USD o ‚ÄúEquivalente USD‚Äù. No crear fila aparte en USD si ya existe la moneda original para el mismo per√≠odo/tipo_registro (ver DEDUP).

Fuente CAF (fuente_etiqueta): etiqueta clara: ‚ÄúCAF Realizado‚Äù, ‚ÄúProyectado (Cronograma)‚Äù, ‚ÄúAnticipo‚Äù, ‚ÄúPago directo‚Äù, ‚ÄúReembolso‚Äù, con referencia de documento (p. ej. ‚Äú(ROP)‚Äù, ‚Äú(INI)‚Äù, ‚Äú(DEC)‚Äù si est√° indicada).

Fecha de √∫ltima revisi√≥n: encabezados/pies o notas (‚Äú√öltima revisi√≥n/Actualizaci√≥n/Fecha del documento/Versi√≥n/Modificado/Revisado el‚Äù).

Nombre del archivo revisado: documento del que proviene el dato final.

Reglas de extracci√≥n y deduplicaci√≥n:

Tabla-primero: prioriza tablas sobre narrativa.

No convertir moneda ni inferir fechas/moneda no sustentadas.

Prioriza moneda original; conserva monto_usd solo si viene expl√≠cito en la tabla (o si no hay registro en moneda original).

DEDUP estricta: no repitas mismo per√≠odo + misma moneda + mismo tipo_registro. Conserva la fila m√°s reciente/consistente y registra diferencias en observacion.

Ambig√ºedad de fecha: usa el literal (Q1 2025, 2026, etc.); no expandas a fechas exactas.

tipo_registro_norm ‚àà {proyectado, realizado} (derivado de la secci√≥n/tabla/fuente).

Niveles de confianza (por campo):

Cada variable se representa como objeto con:

value (string/num o null),

confidence: EXTRAIDO_DIRECTO | EXTRAIDO_INFERIDO | NO_EXTRAIDO,

evidence (cita breve literal o cercana),

Normalizaci√≥n:

fuente_norm (opcional) ‚Üí {CAF Realizado, Proyectado (Cronograma), Anticipo, Pago directo, Reembolso, Transferencia, Giro, null}.

tipo_registro_norm ‚Üí {proyectado, realizado}.

moneda ‚Üí mantener c√≥digo/etiqueta tal como aparece (no normalizar a ISO si no est√° expl√≠cito, salvo equivalentes claros como ‚ÄúUS$‚Äù‚Üí‚ÄúUSD‚Äù).

Reglas de salida (cardinalidad y formato)

Cardinalidad

Detecta todos los registros de desembolso en el/los documento(s).
Por CADA registro identificado (unidad m√≠nima = per√≠odo/fecha √ó moneda √ó tipo_registro):
EMITE UNA (1) instancia del esquema completo ‚ÄúEsquema de salida JSON (por registro)‚Äù.
No agregues ni elimines claves.
Si falta evidencia en una clave: value=null, confidence="NO_EXTRAIDO", evidence=null.
No incluyas texto adicional fuera del JSON.
Mant√©n el orden de claves como en el esquema.

Few-shot compacto (referencial, NO generar)
- Prop√≥sito: ilustrar patrones de extracci√≥n. NO son datos reales ni deben emitirse.
- Cardinalidad del ejemplo: 1 entrada de tabla ‚Üí 1 (una) fila JSON.
- Prohibido: crear filas adicionales por el mismo ejemplo. 

Ejemplo A (realizado en USD) ‚Äî NO EMITIR
Entrada de tabla (una fila):
  2024-06 | USD 1,250,000 | CAF Realizado (DEC)
Salida esperada (una sola fila JSON):
  tipo_registro_norm="realizado";
  fecha_desembolso.value="2024-06";
  monto_original.value=1250000; moneda.value="USD";
  monto_usd.value=1250000 (solo si no existe fila en moneda original);
  fuente_etiqueta.value="CAF Realizado (DEC)".

Ejemplo B (proyectado en moneda local con columna USD) ‚Äî NO EMITIR
Entrada de tabla (una fila):
  Q1 2025 | PEN 3,500,000 | Equivalente USD 920,000 | Proyectado (Cronograma ROP)
Salida esperada (una sola fila JSON):
  tipo_registro_norm="proyectado";
  fecha_desembolso.value="Q1 2025";
  monto_original.value=3500000; moneda.value="PEN";
  monto_usd.value=920000 (si tu esquema conserva columna expl√≠cita);
  fuente_norm="Proyectado (Cronograma)".

Esquema de salida JSON (por registro)
{
  "codigo_CFA": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},

  "tipo_registro": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},
  "tipo_registro_norm": null,  // proyectado | realizado

  "fecha_desembolso": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},

  "monto_original": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},

  "moneda": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},

  "monto_usd": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},

  "fuente_etiqueta": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null },

  "fuente_norm": null,  // CAF Realizado | Proyectado (Cronograma) | Anticipo | Pago directo | Reembolso | Transferencia | Giro | null

  "fecha_extraccion": "YYYY-MM-DD HH:MM",

  "fecha_ultima_revision": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null },

  "nombre_archivo": "ROP_....pdf"
}
'''

PROMPT_PRODUCTOS = ''' 
Prompt ‚Äî Agente Productos 

Eres un Analista de Cartera experto en proyectos CAF.
Debes identificar todos los productos comprometidos en el proyecto y generar las filas solicitadas por cada producto, separando meta num√©rica y unidad, normalizando campos y dejando claro el estado de extracci√≥n.

Importante: no emites concepto final.

Prioridad documental:

Jerarqu√≠a: ROP > INI > DEC > IFS > Anexo Excel (si lo cita el √≠ndice).

En duplicados: usar la versi√≥n m√°s reciente; si cambian valores, registrar en observacion.

Checklist anti‚Äì‚ÄúNO_EXTRAIDO‚Äù:

Tablas/Matrices ‚Üí ‚ÄúMatriz de Indicadores‚Äù, ‚ÄúMarco L√≥gico‚Äù, ‚ÄúMetas f√≠sicas‚Äù.

Narrativo ‚Üí ‚ÄúResultados esperados‚Äù, ‚ÄúComponentes‚Äù, ‚ÄúSeguimiento de indicadores‚Äù (IFS).

Anexos/Excel ‚Üí cuando est√©n citados en √≠ndice.

Encabezados/pies ‚Üí ‚Äú√öltima revisi√≥n/Actualizaci√≥n‚Äù.

D√≥nde buscar (por campo):

C√≥digo CFA / CFX: portada, primeras p√°ginas, marcos l√≥gicos, car√°tulas.

Descripci√≥n de producto: t√≠tulos/filas en matrices, POA, componentes, IFS.

Meta del producto / Meta unidad: columnas de metas ‚Üí separa n√∫mero/unidad (230 km ‚Üí meta_num=230, meta_unidad=km).

Fuente del indicador: columna/nota ‚ÄúFuente‚Äù (ROP, INI, DEC, IFS, SSC).

Fecha cumplimiento de meta: ‚ÄúFecha meta‚Äù, ‚ÄúFecha de cumplimiento‚Äù, ‚ÄúPlazo‚Äù.

Tipo de dato: pendiente/proyectado/realizado (claves: programado, estimado, alcanzado).

Caracter√≠stica: {administraci√≥n, capacitaci√≥n, fortalecimiento institucional, infraestructura}.

Check_producto: ‚ÄúS√≠‚Äù si corresponde inequ√≠vocamente a producto (no resultado).

Fecha √∫ltima revisi√≥n: encabezados/pies.

Reglas especiales:

Validar producto vs resultado ‚Üí no confundir.

Acumulado vs per√≠odo ‚Üí si es acumulativo, no dupliques registros.

Idiomas/formatos ‚Üí aceptar ES/PT/EN y tablas rotadas.

Separaci√≥n meta/unidad ‚Üí reconocer variantes: ‚Äú230 kil√≥metros‚Äù, ‚Äú1.500 personas‚Äù, ‚Äú100%‚Äù, ‚Äú2,5 ha‚Äù.

No inventar ‚Üí si falta meta o unidad, deja null.

Niveles de confianza:

Cada variable incluye:

value (dato literal o null).

confidence: EXTRAIDO_DIRECTO | EXTRAIDO_INFERIDO | NO_EXTRAIDO.

evidence: cita breve literal o cercana.

Normalizaci√≥n:

tipo_dato_norm ‚àà {pendiente, proyectado, realizado, null}.

caracteristica_norm ‚àà {administracion, capacitacion, fortalecimiento institucional, infraestructura, null}.

meta_num: n√∫mero puro (ej. 230 de ‚Äú230 km‚Äù).

meta_unidad_norm: cat√°logo controlado (%, km, personas, m¬≤, m¬≥, horas, hect√°reas, kVA, MVA, l/s, galones, miles gal/d√≠a, toneladas, cantidad/a√±o, miles m¬≤, etc.).

Few-shot (patrones t√≠picos):

‚Äú230 km de carretera‚Äù ‚Üí meta_num=230, meta_unidad_norm=km.

‚Äú1,500 personas capacitadas‚Äù ‚Üí meta_num=1500, meta_unidad_norm=personas, caracteristica_norm=capacitacion.

‚ÄúResultado alcanzado‚Äù ‚Üí tipo_dato_norm=realizado.

‚ÄúMeta programada para 2024‚Äù ‚Üí tipo_dato_norm=proyectado.

‚ÄúTalleres de capacitaci√≥n‚Äù ‚Üí caracteristica_norm=capacitacion.

Reglas de salida:
Detecta todos los productos en el/los documento(s).
Por CADA producto identificado:
EMITE UNA (1) instancia del esquema completo ‚ÄúEsquema de salida JSON (por producto)‚Äù.
No agregues ni elimines claves del esquema.
No mezcles datos de productos distintos en la misma instancia.
No incluyas texto adicional fuera de las l√≠neas JSON.
Mant√©n el orden de las claves tal como est√° definido en ‚ÄúEsquema de salida JSON (por producto)‚Äù.
Si no hay evidencia ‚Üí value=null, confidence="NO_EXTRAIDO".

fecha_extraccion: fecha-hora actual del sistema.
nombre_archivo: documento fuente.

Esquema de salida JSON (por producto)
{
  "codigo_CFA": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null },
  "codigo_CFX": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null },

  "descripcion_producto": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null },

  "meta_producto": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null },
  "meta_unidad": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null },
  "meta_num": null,
  "meta_unidad_norm": null,

  "fuente_indicador": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},
  "fecha_cumplimiento_meta": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null},

  "tipo_dato": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null },
  "tipo_dato_norm": null,

  "caracteristica": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null },
  "caracteristica_norm": null,

  "check_producto": "No",

  "fecha_extraccion": "YYYY-MM-DD HH:MM",
  "fecha_ultima_revision": { "value": null, "confidence": "NO_EXTRAIDO", "evidence": null },

  "nombre_archivo": "ROP_....pdf",
 }
'''

# Configurar logging para reducir verbosidad de Azure
logging.getLogger('azure.core.pipeline.policies.http_logging_policy').setLevel(logging.WARNING)
logging.getLogger('azure').setLevel(logging.WARNING)
logging.getLogger('openai').setLevel(logging.WARNING)

class OpenAIBatchProcessor:
    """
    Procesador de Azure OpenAI Batch API para an√°lisis de documentos.
    Genera batch jobs para procesar documentos y chunks usando 3 prompts espec√≠ficos.
    """
    
    def __init__(self):
        self.logger = get_logger('openai_batch_processor')
        self._setup_client()
        self._load_prompts()
        
    def _setup_client(self):
        """Configura el cliente de Azure OpenAI usando variables de entorno."""
        try:
            # Obtener configuraci√≥n del .env
            api_key = os.getenv('AZURE_OPENAI_API_KEY')
            endpoint = os.getenv('AZURE_OPENAI_ENDPOINT', 'https://oai-poc-idatafactory-cr.openai.azure.com/')
            api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2025-04-01-preview')
            self.deployment_name = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', 'gpt-4o-2')
            
            if not api_key:
                raise ValueError("AZURE_OPENAI_API_KEY no encontrada en variables de entorno")
            
            self.client = AzureOpenAI(
                api_version=api_version,
                azure_endpoint=endpoint,
                api_key=api_key
            )
            
            self.logger.info(f"Cliente Azure OpenAI Batch configurado exitosamente")
            self.logger.info(f"Endpoint: {endpoint}")
            self.logger.info(f"Deployment: {self.deployment_name}")
            
        except Exception as e:
            self.logger.error(f"Error configurando cliente Azure OpenAI: {str(e)}")
            raise
    
    def _load_prompts(self):
        """Carga los prompts desde constantes definidas en el c√≥digo"""
        try:
            # Asignar prompts a atributos de instancia para compatibilidad
            self.prompt_auditoria = PROMPT_AUDITORIA
            self.prompt_desembolsos = PROMPT_DESEMBOLSOS
            self.prompt_productos = PROMPT_PRODUCTOS
            
            # Crear diccionario de prompts para acceso por n√∫mero
            self.prompts = {
                'prompt_1': PROMPT_AUDITORIA,
                'prompt_2': PROMPT_PRODUCTOS, 
                'prompt_3': PROMPT_DESEMBOLSOS
            }
            
            self.logger.info("‚úÖ Prompts cargados exitosamente desde constantes")
            
        except Exception as e:
            self.logger.error(f"Error cargando prompts: {str(e)}")
            raise
    
    def _get_document_prefix(self, document_content: Dict[str, Any]) -> str:
        """
        Extrae el prefijo del documento basado en el nombre del archivo.
        
        Args:
            document_content: Contenido del documento
            
        Returns:
            Prefijo del documento en may√∫sculas (ej: 'IXP', 'ROP', 'INI')
        """
        # Obtener nombre del documento
        document_name = document_content.get('document_name', document_content.get('filename', ''))
        
        # Si es un chunk, obtener el nombre base
        if '_chunk_' in document_name:
            document_name = document_name.split('_chunk_')[0]
        
        # Extraer prefijo (primeras 3 letras antes del primer gui√≥n)
        if '-' in document_name:
            prefix = document_name.split('-')[0].upper()
        else:
            # Si no hay gui√≥n, tomar las primeras 3 letras
            prefix = document_name[:3].upper()
        
        return prefix
    
    def _should_process_with_prompt(self, document_content: Dict[str, Any], prompt_number: int) -> bool:
        """
        Determina si un documento debe ser procesado con un prompt espec√≠fico.
        
        Args:
            document_content: Contenido del documento
            prompt_number: N√∫mero del prompt (1, 2, 3)
            
        Returns:
            True si debe procesarse, False si no
        """
        document_prefix = self._get_document_prefix(document_content)
        
        # Filtros por prefijo seg√∫n prompt
        if prompt_number == 1:  # Auditor√≠a
            allowed_prefixes = ['IXP']
        elif prompt_number == 2:  # Productos
            allowed_prefixes = ['ROP', 'INI', 'DEC', 'IFS']
        elif prompt_number == 3:  # Desembolsos
            allowed_prefixes = ['ROP', 'INI', 'DEC', 'IFS']
        else:
            return False
        
        return document_prefix in allowed_prefixes
    
    def _create_batch_request(self, custom_id: str, prompt: str, content: str) -> Dict[str, Any]:
        """
        Crea una request individual para el batch job.
        
        Args:
            custom_id: ID √∫nico para identificar la request
            prompt: Prompt a usar
            content: Contenido del documento
            
        Returns:
            Dict con la estructura de request para batch
        """
        return {
            "custom_id": custom_id,
            "method": "POST",
            "url": "/chat/completions",
            "body": {
                "model": self.deployment_name,
                "messages": [
                    {
                        "role": "system", 
                        "content": "Eres un Analista experto en documentos de auditor√≠a. Tu tarea es extraer informaci√≥n espec√≠fica siguiendo un formato estructurado y emitiendo conceptos normalizados para entregar en formato JSON lo solicitado."
                    },
                    {
                        "role": "user", 
                        "content": f"{prompt}\n\nDocumento:\n{content}"
                    }
                ],
                "max_completion_tokens": 1000,
                "temperature": 0.3
            }
        }
    
    def create_batch_job(self, project_name: str) -> Dict[str, Any]:
        """
        Crea un batch job para procesar todos los documentos de un proyecto usando BlobStorageClient.
        
        Args:
            project_name: Nombre del proyecto (ej: CFA009660)
            
        Returns:
            Dict con informaci√≥n del batch job creado
        """
        self.logger.info(f"üöÄ Creando batch job para proyecto: {project_name}")
        
        batch_requests = []
        documents_info = []
        # Manifest detallado para auditor√≠a: prompt + contexto por request
        requests_manifest: List[Dict[str, Any]] = []
        
        try:
            blob_client = BlobStorageClient()
            
            # Procesar documentos DI desde blob storage
            di_documents = blob_client.list_processed_documents(project_name)
            for doc_name in di_documents:
                if doc_name.endswith('.json'):
                    self._add_document_to_batch_from_blob(
                        doc_name, project_name, batch_requests, documents_info, blob_client, 'DI', requests_manifest
                    )
            
            # Procesar chunks desde blob storage
            chunk_documents = blob_client.list_chunks(project_name)
            for chunk_name in chunk_documents:
                if chunk_name.endswith('.json'):
                    self._add_document_to_batch_from_blob(
                        chunk_name, project_name, batch_requests, documents_info, blob_client, 'chunks', requests_manifest
                    )
            
            if not batch_requests:
                raise ValueError(f"No se encontraron documentos para procesar en proyecto {project_name}")
            
            # Crear archivo JSONL temporal (formato requerido por Azure Batch API)
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False, encoding='utf-8') as f:
                for request in batch_requests:
                    f.write(json.dumps(request, ensure_ascii=False) + '\n')
                batch_input_file = f.name
            
            self.logger.info(f"üìÑ Archivo batch temporal creado: {batch_input_file} ({len(batch_requests)} requests)")
            
            # Subir archivo a Azure asegurando cierre del handle en Windows
            with open(batch_input_file, "rb") as fh:
                uploaded = self.client.files.create(
                    file=fh,
                    purpose="batch"
                )
            
            # Limpiar archivo temporal (handle ya cerrado)
            try:
                os.unlink(batch_input_file)
            except Exception as e:
                self.logger.warning(f"No se pudo eliminar archivo temporal {batch_input_file}: {str(e)}")
            
            # Crear batch job
            batch = self.client.batches.create(
                input_file_id=uploaded.id,
                endpoint="/chat/completions",
                completion_window="24h",
                metadata={"project": project_name}
            )
            
            batch_info = {
                "batch_id": batch.id,
                "project_name": project_name,
                "input_file_id": uploaded.id,
                "created_at": datetime.now().isoformat(),
                "status": batch.status,
                "total_requests": len(batch_requests),
                "documents_info": documents_info
            }
            
            # Guardar informaci√≥n del batch en blob storage
            batch_info_content = json.dumps(batch_info, indent=2, ensure_ascii=False)
            batch_info_path = f"basedocuments/{project_name}/processed/openai_logs/batch_info_{project_name}_{batch.id}.json"
            blob_client.upload_blob(batch_info_path, batch_info_content)

            # Guardar manifest con prompt + contexto por request (para auditor√≠a)
            try:
                manifest_path = f"basedocuments/{project_name}/processed/openai_logs/batch_payload_{project_name}_{batch.id}.jsonl"
                lines = []
                for item in requests_manifest:
                    lines.append(json.dumps(item, ensure_ascii=False))
                manifest_bytes = ("\n".join(lines)).encode('utf-8')
                blob_client.upload_blob(manifest_path, manifest_bytes, content_type='application/x-ndjson')
                self.logger.info(f"üßæ Manifest de prompts/contexto guardado en blob: {manifest_path}")
            except Exception as e:
                self.logger.warning(f"No se pudo guardar el manifest de requests: {str(e)}")
            
            # Marcar en CosmosDB que el proyecto tiene un batch pendiente (best-effort)
            try:
                sharepoint_folder = os.environ.get("SHAREPOINT_FOLDER")
                container_folder = os.environ.get("COSMOS_CONTAINER_FOLDER")
                if sharepoint_folder and container_folder:
                    doc_id = f"{sharepoint_folder}|{project_name}"
                    cdb = CosmosDBClient()
                    doc = cdb.read_item(doc_id, doc_id, container_folder)
                    if doc is not None:
                        doc["isBatchPending"] = True
                        doc["jobBatchId"] = batch.id
                        cdb.upsert_item(doc, container_folder)
                        self.logger.info(f"CosmosDB folder marked pending: {doc_id}")
                else:
                    self.logger.warning("Cosmos env vars missing (SHAREPOINT_FOLDER/COSMOS_CONTAINER_FOLDER); skipping pending mark")
            except Exception as e:
                self.logger.warning(f"Could not mark CosmosDB pending for project {project_name}: {str(e)}")

            self.logger.info(f"‚úÖ Batch job creado exitosamente:")
            self.logger.info(f"   üìã Batch ID: {batch.id}")
            self.logger.info(f"   üìä Total requests: {len(batch_requests)}")
            self.logger.info(f"   üìÅ Info guardada en blob: {batch_info_path}")
            
            return batch_info
            
        except Exception as e:
            self.logger.error(f"Error creando batch job: {str(e)}")
            raise
    
    def process_chunks(self, chunks: List[Dict], document_name: str, queue_type: str) -> Dict[str, Any]:
        """
        Procesa chunks directamente para crear un batch job.
        
        Args:
            chunks: Lista de chunks del documento
            document_name: Nombre del documento
            queue_type: Tipo de cola (no usado, mantenido por compatibilidad)
            
        Returns:
            Dict con informaci√≥n del batch job creado
        """
        try:
            self.logger.info(f"üöÄ Procesando {len(chunks)} chunks para documento: {document_name}")
            
            batch_requests = []
            documents_info = []
            
            for i, chunk in enumerate(chunks):
                # Crear contenido del chunk con informaci√≥n del documento
                chunk_content = {
                    'content': chunk.get('content', ''),
                    'document_name': document_name,
                    'chunk_index': i
                }
                
                content_text = chunk_content.get('content', '')
                
                doc_info = {
                    "document_name": document_name,
                    "chunk_index": i,
                    "prefix": self._get_document_prefix(chunk_content),
                    "prompts_applied": []
                }
                
                # Verificar y agregar prompts aplicables
                prompts = [
                    (1, "auditoria", self.prompt_auditoria),
                    (2, "desembolsos", self.prompt_desembolsos),
                    (3, "productos", self.prompt_productos)
                ]
                
                for prompt_num, prompt_type, prompt_text in prompts:
                    if self._should_process_with_prompt(chunk_content, prompt_num):
                        custom_id = f"{document_name}_{prompt_type}_chunk_{i:03d}"
                        
                        request = self._create_batch_request(custom_id, prompt_text, content_text)
                        batch_requests.append(request)
                        doc_info["prompts_applied"].append(prompt_type)
                
                if doc_info["prompts_applied"]:
                    documents_info.append(doc_info)
                    self.logger.info(f"üìÑ Chunk {i} agregado al batch (prompts: {doc_info['prompts_applied']})")
            
            if not batch_requests:
                raise ValueError(f"No se generaron requests para el documento {document_name}")
            
            # Crear archivo JSONL temporal
            temp_batch_file = f"/tmp/batch_input_{document_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
            
            with open(temp_batch_file, 'w', encoding='utf-8') as f:
                for request in batch_requests:
                    f.write(json.dumps(request, ensure_ascii=False) + '\n')
            
            self.logger.info(f"üìÑ Archivo batch temporal creado: {temp_batch_file} ({len(batch_requests)} requests)")
            
            # Subir archivo a Azure (asegurar cierre de handle)
            with open(temp_batch_file, "rb") as fh:
                uploaded = self.client.files.create(
                    file=fh,
                    purpose="batch"
                )
            
            # Crear batch job
            batch = self.client.batches.create(
                input_file_id=uploaded.id,
                endpoint="/chat/completions",
                completion_window="24h",
                metadata={"document": document_name}
            )
            
            batch_info = {
                "batch_id": batch.id,
                "document_name": document_name,
                "input_file_id": uploaded.id,
                "input_file_name": temp_batch_file,
                "created_at": datetime.now().isoformat(),
                "status": batch.status,
                "total_requests": len(batch_requests),
                "documents_info": documents_info
            }
            
            self.logger.info(f"‚úÖ Batch job creado exitosamente:")
            self.logger.info(f"   üìã Batch ID: {batch.id}")
            self.logger.info(f"   üìä Total requests: {len(batch_requests)}")
            
            # Limpiar archivo temporal
            try:
                os.remove(temp_batch_file)
            except:
                pass

            # Best-effort: marcar en Cosmos que el proyecto/documento tiene batch pendiente
            try:
                # Si los chunks vienen del flujo de documento individual, inferir project_name del document_name si es posible
                project_name = os.environ.get("CURRENT_PROJECT_NAME")  # opcional si se setea antes
                # No siempre tenemos proyecto aqu√≠; por compatibilidad, no forzar.
                sharepoint_folder = os.environ.get("SHAREPOINT_FOLDER")
                container_folder = os.environ.get("COSMOS_CONTAINER_FOLDER")
                if project_name and sharepoint_folder and container_folder:
                    doc_id = f"{sharepoint_folder}|{project_name}"
                    cdb = CosmosDBClient()
                    doc = cdb.read_item(doc_id, doc_id, container_folder)
                    if doc is not None:
                        doc["isBatchPending"] = True
                        doc["jobBatchId"] = batch.id
                        cdb.upsert_item(doc, container_folder)
                        self.logger.info(f"CosmosDB folder marked pending (chunks): {doc_id}")
            except Exception as e:
                self.logger.warning(f"Could not mark CosmosDB pending (chunks): {str(e)}")

            return batch_info
            
        except Exception as e:
            self.logger.error(f"Error procesando chunks: {str(e)}")
            raise

    def _add_document_to_batch_from_blob(self, doc_name: str, project_name: str, batch_requests: List[Dict], documents_info: List[Dict], blob_client: BlobStorageClient, doc_type: str, requests_manifest: Optional[List[Dict[str, Any]]] = None):
        """
        A√±ade un documento desde blob storage al batch job.
        
        Args:
            doc_name: Nombre del documento
            project_name: Nombre del proyecto
            batch_requests: Lista de requests del batch
            documents_info: Lista de informaci√≥n de documentos
            blob_client: Cliente de blob storage
            doc_type: Tipo de documento ('DI' o 'chunks')
        """
        try:
            # Determinar la subcarpeta seg√∫n el tipo
            if doc_type == 'DI':
                subfolder = 'DI'
            else:  # chunks
                subfolder = 'chunks'
            
            # Descargar contenido del blob usando el m√©todo correcto
            doc_data = blob_client.load_processed_document(project_name, subfolder, doc_name)
            if not doc_data:
                self.logger.warning(f"No se pudo descargar el documento: {doc_name} desde {subfolder}")
                return
            
            # Asegurar que el nombre del documento est√© en los datos
            if 'document_name' not in doc_data and 'filename' not in doc_data:
                doc_data['document_name'] = doc_name
            
            # Obtener prefijo del documento
            prefix = self._get_document_prefix(doc_data)
            self.logger.info(f"üîç Documento: {doc_name}, Prefijo extra√≠do: {prefix}")
            
            # Procesar con cada prompt seg√∫n las reglas
            prompts_applied = []
            
            for prompt_num in [1, 2, 3]:
                should_process = self._should_process_with_prompt(doc_data, prompt_num)
                self.logger.info(f"üìã Prompt {prompt_num} para {doc_name}: {'‚úÖ S√ç' if should_process else '‚ùå NO'}")
                if should_process:
                    custom_id = f"{project_name}_{Path(doc_name).stem}_prompt{prompt_num}"
                    prompt = self.prompts[f'prompt_{prompt_num}']
                    content = doc_data.get('content', '')
                    
                    batch_request = self._create_batch_request(custom_id, prompt, content)
                    batch_requests.append(batch_request)
                    
                    # Mapear n√∫mero de prompt a tipo
                    prompt_types = {1: "auditoria", 2: "productos", 3: "desembolsos"}
                    prompts_applied.append(prompt_types[prompt_num])
                    
                    # Agregar entrada al manifest (prompt + contexto)
                    if requests_manifest is not None:
                        requests_manifest.append({
                            "custom_id": custom_id,
                            "prompt_number": prompt_num,
                            "prompt_type": prompt_types[prompt_num],
                            "prompt_text": prompt,
                            "context": content,
                            "document_name": doc_name,
                            "document_type": doc_type,
                            "subfolder": subfolder,
                            "prefix": prefix,
                            "created_at": datetime.now().isoformat()
                        })
                    
                    self.logger.info(f"üìÑ A√±adido al batch: {custom_id} (Prefijo: {prefix})")
            
            # A√±adir informaci√≥n del documento solo si se aplicaron prompts
            if prompts_applied:
                documents_info.append({
                    "document_name": doc_name,
                    "document_type": doc_type,
                    "subfolder": subfolder,
                    "prefix": prefix,
                    "prompts_applied": prompts_applied,
                    "processed_at": datetime.now().isoformat()
                })
                self.logger.info(f"üìÑ Documento agregado: {doc_name} (prompts: {prompts_applied})")
            else:
                self.logger.info(f"‚è≠Ô∏è Saltando documento: {doc_name} (no aplica ning√∫n prompt)")
            
        except Exception as e:
            self.logger.error(f"Error procesando documento {doc_name}: {str(e)}")
            raise

    def _add_document_to_batch(self, doc_path: str, project_name: str, batch_requests: List[Dict], documents_info: List[Dict]):
        """
        Agrega un documento al batch con los prompts aplicables.
        
        Args:
            doc_path: Ruta al documento
            project_name: Nombre del proyecto
            batch_requests: Lista de requests del batch
            documents_info: Lista de informaci√≥n de documentos
        """
        try:
            # Cargar contenido del documento
            with open(doc_path, 'r', encoding='utf-8') as f:
                document_content = json.load(f)
            
            document_content['project_name'] = project_name
            content_text = document_content.get('content', '')
            
            # Extraer informaci√≥n del documento
            document_name = document_content.get('document_name', document_content.get('filename', Path(doc_path).stem))
            chunk_index = None
            if '_chunk_' in str(doc_path):
                chunk_match = re.search(r'_chunk_(\d+)', str(doc_path))
                if chunk_match:
                    chunk_index = int(chunk_match.group(1))
            
            doc_info = {
                "document_name": document_name,
                "file_path": doc_path,
                "chunk_index": chunk_index,
                "prefix": self._get_document_prefix(document_content),
                "prompts_applied": []
            }
            
            # Verificar y agregar prompts aplicables
            prompts = [
                (1, "auditoria", self.prompt_auditoria),
                (2, "productos", self.prompt_productos),
                (3, "desembolsos", self.prompt_desembolsos)
            ]
            
            for prompt_num, prompt_type, prompt_text in prompts:
                if self._should_process_with_prompt(document_content, prompt_num):
                    custom_id = f"{project_name}_{document_name}_{prompt_type}"
                    if chunk_index is not None:
                        custom_id += f"_chunk_{chunk_index:03d}"
                    
                    request = self._create_batch_request(custom_id, prompt_text, content_text)
                    batch_requests.append(request)
                    doc_info["prompts_applied"].append(prompt_type)
            
            if doc_info["prompts_applied"]:
                documents_info.append(doc_info)
                self.logger.info(f"üìÑ Agregado al batch: {document_name} (prompts: {doc_info['prompts_applied']})")
            else:
                self.logger.info(f"‚è≠Ô∏è Saltando documento: {document_name} (no aplica ning√∫n prompt)")
                
        except Exception as e:
            self.logger.error(f"Error procesando documento {doc_path}: {str(e)}")
            raise
